{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Social Network Analysis [CT0540]\n",
    "---\n",
    "**Project:** An Insight into Twitter Networks of Central Banks.\n",
    "\n",
    "**Author:** [`André Ramolivaz`](https://www.linkedin.com/in/andreramolivaz/)\n",
    "\n",
    "**Supervisor:** `Fabiana Zollo`\n",
    "\n",
    "**Initial Analysis Structure:**\n",
    "</br>\n",
    "</br>\n",
    "\n",
    "    CT0540\n",
    "    ├── General overview\n",
    "    │   ├── ✅ [Presence Central Banks on social media]()\n",
    "    │   ├── ✅ Our case study (analysis on BCE, BoE, FED)\n",
    "    │   └── ✅️ Network analysis (networks graphs based on followers and tweet mentions, clustering type of user)\n",
    "    ├── Tweet analysis (not only from official account)\n",
    "    │   ├── ✅ Sentiment Analysis\n",
    "    │   ├── ✅ Topic Modeling\n",
    "    │   │   ├── ✅ ECB, BoE, FED\n",
    "    │   │   └── ✅️ All\n",
    "    │   ├── ✅ Hashtag analysis\n",
    "    │   └── ✅ Engagement metrics\n",
    "    └── Case study (whatever it takes)\n",
    "        ├── ✅️ Sentiment analysis 15 days before\n",
    "        ├── ✅️ Sentiment analysis 15 days after\n",
    "        ├── ✅️ Main tweets\n",
    "        └── ✅️ Correlation with stock market\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import csv\n",
    "from tabulate import tabulate\n",
    "import snscrape.modules.twitter as tw\n",
    "import time\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import gensim\n",
    "from gensim import corpora, models\n",
    "import networkx as nx\n",
    "import plotly.graph_objs as go\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = \"notebook\"\n",
    "import tweepy\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from pychord import Chord\n",
    "from networkx.algorithms.community import greedy_modularity_communities\n",
    "import community.community_louvain\n",
    "from textblob import TextBlob\n",
    "from langdetect import detect\n",
    "import re\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    " **General analysis about the presence of Central Banks on social media**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "central_banks_followers = [[\"European Central Bank\", 732300, 428574, 73100],\n",
    "                           [\"Federal Reserve\", 1032459, 111032, 197000],\n",
    "                           [\"Bank of England\", 392400, 238169, 29700],\n",
    "                           [\"Bank of Canada\", 226000, 128079, 11500],\n",
    "                           [\"Reserve Bank of Australia\", 64900, 120415, 7610],\n",
    "                           [\"Banque de France \", 51000, 181391, 20000],\n",
    "                           [\"Banca d'Italia\", 20900, 109047, 12500],\n",
    "                           [\"Banco de Mexico\", 913500, 46631, 43000],\n",
    "                           [\"Reserve Bank of India\", 1900000, 23050, 166000],\n",
    "                           [\"Central Bank of the Republic of Turkey\", 39600, 56228, 13000],\n",
    "                           [\"Central Bank of Argentina\", 153300, 129428, 10200],\n",
    "                           [\"Central Bank of Brazil\", 454600, 417269, 114000],\n",
    "                           [\"Monetary Authority of Singapore\", 25400, 119347, 2620],\n",
    "                           [\"Reserve Bank of New Zealand\", 13900, 32917, 5120],\n",
    "                           [\"South African Reserve Bank\", 112100, 159563, 2610],]\n",
    "\n",
    "df = pd.DataFrame(central_banks_followers, columns=[\"Bank\", \"Twitter\", \"LinkedIn\", \"YouTube\"])\n",
    "print(tabulate(df, headers = [\"Central Bank\", \"Twitter\", \"Linkedin\", \"Youtube\", \"Instagram\"], tablefmt='fancy_grid', showindex=\"never\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Twitter Ratio\"] = df[\"Twitter\"] / df[\"Twitter\"].sum()\n",
    "df[\"LinkedIn Ratio\"] = df[\"LinkedIn\"] / df[\"LinkedIn\"].sum()\n",
    "df[\"YouTube Ratio\"] = df[\"YouTube\"] / df[\"YouTube\"].sum()\n",
    "\n",
    "df.sort_values(by=\"Twitter Ratio\", ascending=False, inplace=True)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.bar(df[\"Bank\"], df[\"Twitter Ratio\"], color=\"blue\")\n",
    "ax.bar(df[\"Bank\"], df[\"LinkedIn Ratio\"], color=\"green\", bottom=df[\"Twitter Ratio\"])\n",
    "ax.bar(df[\"Bank\"], df[\"YouTube Ratio\"], color=\"red\", bottom=df[\"Twitter Ratio\"] + df[\"LinkedIn Ratio\"])\n",
    "\n",
    "ax.set_xlabel(\"Central Bank\")\n",
    "ax.set_ylabel(\"Ratio of Followers\")\n",
    "ax.set_title(\"Central Banks' Followers on Social Media\")\n",
    "plt.xticks(rotation=90)\n",
    "ax.legend(labels=[\"Twitter\", \"LinkedIn\", \"YouTube\"], loc=\"upper right\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ecb_tweets = pd.read_csv('ecb_tweets.csv')\n",
    "boe_tweets = pd.read_csv('bankofengland_tweets.csv')\n",
    "fed_tweets = pd.read_csv('federalreserve_tweets.csv')\n",
    "\n",
    "all_tweets = pd.concat([ecb_tweets, boe_tweets, fed_tweets])\n",
    "\n",
    "all_tweets['tweet_date'] = pd.to_datetime(all_tweets['tweet_date'])\n",
    "\n",
    "like_weight = 1\n",
    "retweet_weight = 1.5\n",
    "\n",
    "# Define a function to calculate the score using the weighted sum formula\n",
    "def calculate_score(row):\n",
    "    return (like_weight * row['like_count']) + (retweet_weight * row['retweet_count'])\n",
    "\n",
    "# Apply the function to create a new column with the score\n",
    "all_tweets['count_score'] = all_tweets.apply(calculate_score, axis=1)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.title('Social Media Metrics')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Count Score')\n",
    "plt.grid()\n",
    "\n",
    "# Plot the data for each bank\n",
    "for name, group in all_tweets.groupby('username'):\n",
    "    if name == 'ecb':\n",
    "        plt.plot(group['tweet_date'], group['count_score'], color='blue', linestyle='-', alpha=0.8, label='ECB')\n",
    "    elif name == 'bankofengland':\n",
    "        plt.plot(group['tweet_date'], group['count_score'], color='green', linestyle='--', alpha=0.8, label='BoE')\n",
    "    elif name == 'federalreserve':\n",
    "        plt.plot(group['tweet_date'], group['count_score'], color='red', linestyle=':', alpha=0.8, label='Fed')\n",
    "\n",
    "# Set the X-axis limits to the earliest and latest dates in the dataset\n",
    "plt.xlim(all_tweets['tweet_date'].min(), all_tweets['tweet_date'].max())\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**Collecting followers from ECB, BoE, FED for friends connection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "api_key = \"ScV72nngWg1H0EOx0feRqbwCA\"\n",
    "api_key_secret = \"SDe4LbAKTAAi7SRcmIX37irQr8aubWMRu7eLb0U1fVSIKaCWDK\"\n",
    "bearer_token = \"AAAAAAAAAAAAAAAAAAAAAOQ6VwEAAAAAtGKOf6bobogb5JEWiF6bmLmLkuA%3D5R9TCklAx9It9mBq87sXiieuBn9GzpexGmKBXviw8enl88AzYR\"\n",
    "access_token = \"750009430540943360-70xlf5Ryr7zfC03Z5PA4BgWQ3jh4umU\"\n",
    "access_token_secret = \"dOKhLbCEG0NawHIFT8SpInQwpd4DuZrpkwpKDVvmtYLL2\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_followers_csv(username, filename):\n",
    "\n",
    "    # Authenticate the Twitter API with your API credentials\n",
    "    auth = tweepy.OAuthHandler(api_key, api_key_secret)\n",
    "    auth.set_access_token(access_token, access_token_secret)\n",
    "    api = tweepy.API(auth, wait_on_rate_limit=True)\n",
    "\n",
    "    # Get the follower IDs of the specified Twitter username\n",
    "    follower_ids = api.get_follower_ids(screen_name=username)\n",
    "\n",
    "    # Randomly select 1,000 follower IDs\n",
    "    if (username == \"ecb\"):\n",
    "        random_ids = random.sample(follower_ids, 732)\n",
    "    if (username == \"bankofengland\"):\n",
    "        random_ids = random.sample(follower_ids, 392)\n",
    "    if (username == \"federalreserve\"):\n",
    "        random_ids = random.sample(follower_ids, 1032)\n",
    "\n",
    "    # Lookup the corresponding followers of the selected IDs\n",
    "    followers = []\n",
    "    for follower_id in random_ids:\n",
    "        follower = api.get_user(user_id=follower_id)\n",
    "        followers.append([follower.screen_name, follower.name])\n",
    "\n",
    "    with open(filename, 'w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['Username', 'Name'])\n",
    "        writer.writerows(followers)\n",
    "\n",
    "    print(f'{len(followers)} followers saved to {filename}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "get_followers_csv('ecb', 'ecb_followers.csv')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "get_followers_csv('federalreserve', 'federalreserve_followers.csv')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "get_followers_csv('bankofengland', 'bankofengland_followers.csv')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**Collecting tweets from ECB, BoE, FED**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tweets_csv(username, event_date, days_before, days_after, general):\n",
    "\n",
    "    event_timestamp = time.mktime(time.strptime(event_date, \"%Y-%m-%d\"))\n",
    "    start_timestamp = event_timestamp - (days_before * 24 * 60 * 60)\n",
    "    end_timestamp = event_timestamp + (days_after * 24 * 60 * 60)\n",
    "\n",
    "    start_date = time.strftime(\"%Y-%m-%d\", time.gmtime(start_timestamp))\n",
    "    end_date = time.strftime(\"%Y-%m-%d\", time.gmtime(end_timestamp))\n",
    "\n",
    "    if(general == 0):\n",
    "        filename = username+\"_tweets.csv\"\n",
    "    if(general == 1):\n",
    "        filename = username+\"_gnrl_tweets.csv\"\n",
    "    if(general == 2):\n",
    "        filename = \"15_days_before.csv\"\n",
    "    if(general == 3):\n",
    "        filename = \"15_days_after.csv\"\n",
    "\n",
    "\n",
    "    with open(filename, \"w\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"conversation_id\",\"username\", \"tweet_text\", \"tweet_date\", \"retweet_count\", \"like_count\", \"view_count\", \"cash_tags\", \"hashtags\", \"mentionned_users\", \"quoted_tweet\", \"place\", \"vibe\", \"card\"])\n",
    "\n",
    "        if(username == \"ecb\" and general == 1):\n",
    "            query = f\"( from:{username} OR #ecb OR #europeancentralbank) since:{start_date} until:{end_date}\"\n",
    "\n",
    "        if(username == \"ecb\" and general == 0):\n",
    "            query = f\" from:{username} ) since:{start_date} until:{end_date}\"\n",
    "\n",
    "        if(username == \"bankofengland\" and general == 1):\n",
    "            query = f\"(from:{username} OR #boe OR #bankofengland) since:{start_date} until:{end_date}\"\n",
    "\n",
    "        if(username == \"bankofengland\" and general == 0):\n",
    "            query = f\"from:{username} since:{start_date} until:{end_date}\"\n",
    "\n",
    "        if(username == \"federalreserve\"and general == 1):\n",
    "            query = f\"(from:{username} OR #fed OR #federalreserve) since:{start_date} until:{end_date}\"\n",
    "\n",
    "        if(username == \"federalreserve\"and general == 0):\n",
    "            query = f\"from:{username} since:{start_date} until:{end_date}\"\n",
    "\n",
    "        if(general > 1):\n",
    "            query = f\"(#ecb OR #europeancentralbank OR #draghi OR #bce OR (#whateverittakes AND #ecb) since:{start_date} until:{end_date}\"\n",
    "\n",
    "\n",
    "        tweets = tw.TwitterSearchScraper(query=query).get_items()\n",
    "\n",
    "        for tweet in tweets:\n",
    "\n",
    "            writer.writerow([tweet.conversationId, tweet.username, tweet.content, tweet.date, tweet.retweetCount, tweet.likeCount, tweet.viewCount, tweet.cashtags, tweet.hashtags,tweet.mentionedUsers, tweet.quotedTweet, tweet.place, tweet.vibe, tweet.card])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From official account:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_tweets_csv(\"ecb\",\"2020-01-01\",1152,1152,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_tweets_csv(\"bankofengland\",\"2020-01-01\",1152,1152,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_tweets_csv(\"federalreserve\",\"2020-01-01\",1152,1152,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From official account and related hashtag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_tweets_csv(\"ecb\",\"2022-01-01\",182,182,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_tweets_csv(\"bankofengland\",\"2022-01-01\",182,182,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "get_tweets_csv(\"federalreserve\",\"2022-01-01\",182,182,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**Tweets analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_analysis (username, general):\n",
    "\n",
    "    if (general == 0):\n",
    "        df = pd.read_csv(username+\"_tweets.csv\")\n",
    "    else:\n",
    "        df = pd.read_csv(username+\"_gnrl_tweets.csv\")\n",
    "\n",
    "    df[\"tweet_text\"] = df[\"tweet_text\"].str.lower()\n",
    "\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    df[\"tweet_text\"] = df[\"tweet_text\"].apply(lambda x: \" \".join([word for word in x.split() if word not in stop_words]))\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    df[\"tweet_text\"] = df[\"tweet_text\"].apply(lambda x: \" \".join([lemmatizer.lemmatize(word) for word in x.split()]))\n",
    "\n",
    "    # create custom sentiment dictionaries for finance, economics, and banking\n",
    "    finance_pos = [\"profit\", \"gains\", \"growth\", \"success\", \"advantage\", \"benefit\", \"wealth\", \"prosperity\"]\n",
    "    finance_neg = [\"loss\", \"decline\", \"failure\", \"disadvantage\", \"risk\", \"debt\", \"recession\", \"deficit\"]\n",
    "    economics_pos = [\"development\", \"progress\", \"prosperity\", \"investment\", \"stability\", \"boom\"]\n",
    "    economics_neg = [\"decline\", \"recession\", \"unemployment\", \"poverty\", \"debt\", \"inflation\"]\n",
    "    banking_pos = [\"savings\", \"interest\", \"loan\", \"credit\", \"debit\", \"bank\", \"finance\"]\n",
    "    banking_neg = [\"fraud\", \"scam\", \"bankruptcy\", \"default\", \"penalty\", \"overdraft\", \"fee\"]\n",
    "\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "    def get_sentiment_scores(text):\n",
    "        scores = sia.polarity_scores(text)\n",
    "        for word in text.split():\n",
    "            if word in finance_pos:\n",
    "                scores[\"pos\"] += 0.2\n",
    "            elif word in finance_neg:\n",
    "                scores[\"neg\"] += 0.2\n",
    "            elif word in economics_pos:\n",
    "                scores[\"pos\"] += 0.1\n",
    "            elif word in economics_neg:\n",
    "                scores[\"neg\"] += 0.1\n",
    "            elif word in banking_pos:\n",
    "                scores[\"pos\"] += 0.3\n",
    "            elif word in banking_neg:\n",
    "                scores[\"neg\"] += 0.3\n",
    "        return scores\n",
    "\n",
    "    df[[\"sentiment_neg\", \"sentiment_neu\", \"sentiment_pos\", \"sentiment_compound\"]] = df[\"tweet_text\"].apply(get_sentiment_scores).apply(pd.Series)\n",
    "\n",
    "    def get_sentiment_label(compound_score):\n",
    "        if compound_score > 0.05:\n",
    "            return \"positive\"\n",
    "        elif compound_score < -0.05:\n",
    "            return \"negative\"\n",
    "        else:\n",
    "            return \"neutral\"\n",
    "\n",
    "    df[\"sentiment_label\"] = df[\"sentiment_compound\"].apply(get_sentiment_label)\n",
    "    if (general == 0):\n",
    "        fig, ax = plt.subplots(figsize=(8,6))\n",
    "        sns.histplot(df, x=\"sentiment_compound\", hue=\"sentiment_label\", kde=True, ax=ax, bins=50)\n",
    "        plt.title(\"Distribution of Sentiment Scores\")\n",
    "        plt.xlabel(\"Sentiment Compound Score\")\n",
    "        plt.ylabel(\"Count\")\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "        df[\"tweet_date\"] = pd.to_datetime(df[\"tweet_date\"])\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(8,6))\n",
    "        sns.scatterplot(data=df, x=\"tweet_date\", y=\"sentiment_compound\", hue=\"sentiment_label\", ax=ax)\n",
    "        plt.title(\"Sentiment Scores Over Time\")\n",
    "        plt.xlabel(\"Date\")\n",
    "        plt.ylabel(\"Sentiment Compound Score\")\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    emotions = [\"anger\", \"anticipation\", \"disgust\", \"fear\", \"joy\", \"sadness\", \"surprise\", \"trust\"]\n",
    "\n",
    "    emotion_freq = {}\n",
    "    for emotion in emotions:\n",
    "        emotion_freq[emotion] = df[\"tweet_text\"].apply(lambda x: emotion in x).sum()\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8,6))\n",
    "    sns.barplot(x=list(emotion_freq.values()), y=list(emotion_freq.keys()), ax=ax, color=\"b\")\n",
    "    plt.title(\"Frequency of Emotions in Tweets\")\n",
    "    plt.xlabel(\"Frequency\")\n",
    "    plt.ylabel(\"Emotion\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def topic_modeling_all ():\n",
    "    df = pd.read_csv(\"ecb_tweets.csv\")\n",
    "\n",
    "    # Preprocess the tweet text\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    def preprocess_text(text):\n",
    "        # Convert to lowercase\n",
    "        text = text.lower()\n",
    "        # Tokenize the text\n",
    "        tokens = word_tokenize(text)\n",
    "        # Remove stop words\n",
    "        tokens = [token for token in tokens if token not in stop_words]\n",
    "        # Remove punctuation\n",
    "        tokens = [token for token in tokens if token.isalpha()]\n",
    "        return tokens\n",
    "\n",
    "    # Tokenize and preprocess the tweet text\n",
    "    df['tokens'] = df['tweet_text'].apply(preprocess_text)\n",
    "\n",
    "    # Create a dictionary of all the tokens\n",
    "    dictionary = corpora.Dictionary(df['tokens'])\n",
    "\n",
    "    # Filter out low-frequency and high-frequency words\n",
    "    dictionary.filter_extremes(no_below=5, no_above=0.5)\n",
    "\n",
    "    # Create a corpus of bag-of-words for each tweet\n",
    "    corpus = [dictionary.doc2bow(doc) for doc in df['tokens']]\n",
    "\n",
    "    # Build a Latent Dirichlet Allocation (LDA) model with 10 topics\n",
    "    lda_model1 = gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=dictionary, num_topics=10, passes=10)\n",
    "\n",
    "\n",
    "    df = pd.read_csv(\"ecb_tweets.csv\")\n",
    "\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "    # Tokenize and preprocess the tweet text\n",
    "    df['tokens'] = df['tweet_text'].apply(preprocess_text)\n",
    "\n",
    "    # Create a dictionary of all the tokens\n",
    "    dictionary = corpora.Dictionary(df['tokens'])\n",
    "\n",
    "    # Filter out low-frequency and high-frequency words\n",
    "    dictionary.filter_extremes(no_below=5, no_above=0.5)\n",
    "\n",
    "    # Create a corpus of bag-of-words for each tweet\n",
    "    corpus = [dictionary.doc2bow(doc) for doc in df['tokens']]\n",
    "\n",
    "    # Build a Latent Dirichlet Allocation (LDA) model with 10 topics\n",
    "    lda_model2 = gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=dictionary, num_topics=10, passes=10)\n",
    "\n",
    "    df = pd.read_csv(\"ecb_tweets.csv\")\n",
    "\n",
    "    # Preprocess the tweet text\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "    # Tokenize and preprocess the tweet text\n",
    "    df['tokens'] = df['tweet_text'].apply(preprocess_text)\n",
    "\n",
    "    # Create a dictionary of all the tokens\n",
    "    dictionary = corpora.Dictionary(df['tokens'])\n",
    "\n",
    "    # Filter out low-frequency and high-frequency words\n",
    "    dictionary.filter_extremes(no_below=5, no_above=0.5)\n",
    "\n",
    "    # Create a corpus of bag-of-words for each tweet\n",
    "    corpus = [dictionary.doc2bow(doc) for doc in df['tokens']]\n",
    "\n",
    "    # Build a Latent Dirichlet Allocation (LDA) model with 10 topics\n",
    "    lda_model3 = gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=dictionary, num_topics=10, passes=10)\n",
    "\n",
    "    G = nx.Graph()\n",
    "    lda_models = [lda_model1, lda_model2, lda_model3]\n",
    "\n",
    "\n",
    "    banks = [\"ECB\", \"FED\", \"BoE\"]\n",
    "    # Add the nodes and edges from all LDA models\n",
    "    for m, lda_model in enumerate(lda_models):\n",
    "        for i, topic in lda_model.show_topics(num_topics=-1, num_words=10, formatted=False):\n",
    "            topic_label = f\"{banks[m]}T{i}\"\n",
    "            G.add_node(topic_label, size=500, color='red')\n",
    "            for word, weight in topic:\n",
    "                word_label = word\n",
    "                G.add_node(word_label, size=weight*500, color='blue')\n",
    "                G.add_edge(topic_label, word_label, weight=weight)\n",
    "\n",
    "    # Define node position using a Fruchterman-Reingold layout\n",
    "    pos = nx.layout.fruchterman_reingold_layout(G)\n",
    "\n",
    "    # Convert the network graph to a pandas dataframe\n",
    "    df = pd.DataFrame(G.edges(data=True), columns=['source', 'target', 'weight'])\n",
    "    df['weight'] = df['weight'].map(lambda x: x['weight'])\n",
    "\n",
    "    # Add edges to the graph from the dataframe\n",
    "    for index, row in df.iterrows():\n",
    "        G.add_edge(row['source'], row['target'], weight=row['weight'])\n",
    "\n",
    "    # Compute the communities using the Louvain algorithm\n",
    "    partition = community.community_louvain.best_partition(G)\n",
    "    communities = [[] for _ in range(max(partition.values()) + 1)]\n",
    "    for node, community_id in partition.items():\n",
    "        communities[community_id].append(node)\n",
    "\n",
    "    color_map = []\n",
    "    size_map = []\n",
    "    for node in G.nodes():\n",
    "        if node.startswith('M'):\n",
    "            color_map.append('red')\n",
    "            size_map.append(500)\n",
    "        else:\n",
    "            community_id = partition[node]\n",
    "            color = plt.cm.Set2(np.mod(community_id, 8) / 8.)\n",
    "            color_map.append(color)\n",
    "            size_map.append(200)\n",
    "\n",
    "    # Define edge width based on the weight\n",
    "    width_map = [d['weight']*10 for (u,v,d) in G.edges(data=True)]\n",
    "\n",
    "    # Define the positions of the nodes using the spring layout algorithm\n",
    "    pos = nx.spring_layout(G, k=0.5, iterations=50)\n",
    "    # Detect communities in the graph using the Louvain algorithm\n",
    "    partition = community.community_louvain.best_partition(G)\n",
    "\n",
    "    color_map = [partition.get(node) for node in G.nodes()]\n",
    "\n",
    "    # Define node size based on whether it's a topic or a word\n",
    "    size_map = [500 if n.startswith('S') else 200 for n in G.nodes()]\n",
    "\n",
    "    # Define edge width based on the weight\n",
    "    width_map = [d['weight']*10 for (u,v,d) in G.edges(data=True)]\n",
    "\n",
    "    nx.draw_networkx_nodes(G, pos, node_color=color_map, node_size=size_map )\n",
    "    nx.draw_networkx_edges(G, pos, width=width_map, edge_color='black')\n",
    "    nx.draw_networkx_labels(G, pos, font_size=5, font_family='sans-serif')\n",
    "\n",
    "    plt.axis('off')\n",
    "    plt.title('Topic Modeling Network Graph')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    print (\"Some statistics:\")\n",
    "    print (nx.info(G))\n",
    "    print (nx.number_of_nodes(G))\n",
    "    print (nx.density(G))\n",
    "    print (nx.is_connected(G))\n",
    "    print (nx.average_shortest_path_length(G))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Compute the communities using the Louvain algorithm\n",
    "    partition = community.community_louvain.best_partition(G)\n",
    "    communities = [[] for _ in range(max(partition.values()) + 1)]\n",
    "    for node, community_id in partition.items():\n",
    "        communities[community_id].append(node)\n",
    "\n",
    "    # Create a matrix of edge weights between communities\n",
    "    n = len(communities)\n",
    "    matrix = np.zeros((n, n))\n",
    "    for u, v, data in G.edges(data=True):\n",
    "        weight = data['weight']\n",
    "        i = partition[u]\n",
    "        j = partition[v]\n",
    "        matrix[i, j] += weight\n",
    "        matrix[j, i] += weight\n",
    "\n",
    "    # Plot the chord diagram\n",
    "    fig, ax = plt.subplots(figsize=(8,8))\n",
    "    ax.matshow(matrix, cmap=plt.cm.Blues)\n",
    "\n",
    "    ticks = np.arange(n)\n",
    "    ax.set_xticks(ticks)\n",
    "    ax.set_xticklabels(communities, rotation=90)\n",
    "    ax.set_yticks(ticks)\n",
    "    ax.set_yticklabels(communities)\n",
    "\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            text = ax.text(j, i, int(matrix[i, j]),\n",
    "                           ha=\"center\", va=\"center\", color=\"white\")\n",
    "\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topic_modeling_3D (username):\n",
    "    df = pd.read_csv(username+\"_tweets.csv\")\n",
    "\n",
    "    # Preprocess the tweet text\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    def preprocess_text(text):\n",
    "        # Convert to lowercase\n",
    "        text = text.lower()\n",
    "        # Tokenize the text\n",
    "        tokens = word_tokenize(text)\n",
    "        # Remove stop words\n",
    "        tokens = [token for token in tokens if token not in stop_words]\n",
    "        # Remove punctuation\n",
    "        tokens = [token for token in tokens if token.isalpha()]\n",
    "        return tokens\n",
    "\n",
    "    # Tokenize and preprocess the tweet text\n",
    "    df['tokens'] = df['tweet_text'].apply(preprocess_text)\n",
    "\n",
    "    # Create a dictionary of all the tokens\n",
    "    dictionary = corpora.Dictionary(df['tokens'])\n",
    "\n",
    "    # Filter out low-frequency and high-frequency words\n",
    "    dictionary.filter_extremes(no_below=5, no_above=0.5)\n",
    "\n",
    "    # Create a corpus of bag-of-words for each tweet\n",
    "    corpus = [dictionary.doc2bow(doc) for doc in df['tokens']]\n",
    "\n",
    "    # Build a Latent Dirichlet Allocation (LDA) model with 10 topics\n",
    "    lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=dictionary, num_topics=10, passes=10)\n",
    "\n",
    "    topic_results = pd.DataFrame()\n",
    "    # Create a network graph of topics and their top words\n",
    "    G = nx.Graph()\n",
    "    for i, topic in lda_model.show_topics(num_topics=-1, num_words=10, formatted=False):\n",
    "        topic_label = \"Topic \" + str(i)\n",
    "        G.add_node(topic_label, size=500, color='red')\n",
    "        for word, weight in topic:\n",
    "            word_label = word\n",
    "            G.add_node(word_label, size=weight*500, color='blue')\n",
    "            G.add_edge(topic_label, word_label, weight=weight)\n",
    "\n",
    "    # Define the node positions using a Fruchterman-Reingold layout\n",
    "    pos = nx.layout.fruchterman_reingold_layout(G)\n",
    "\n",
    "    # Convert the network graph to a pandas dataframe\n",
    "    df = pd.DataFrame(G.edges(data=True), columns=['source', 'target', 'weight'])\n",
    "    df['weight'] = df['weight'].map(lambda x: x['weight'])\n",
    "\n",
    "    # Create a 3D visualization using plotly\n",
    "    edge_trace = go.Scatter3d(x=[], y=[], z=[], mode='lines', line=dict(color='#888', width=3))\n",
    "    node_trace = go.Scatter3d(x=[], y=[], z=[], mode='markers', marker=dict(symbol='circle', size=10, color=[], colorscale='Viridis', line=dict(color='rgb(50,50,50)', width=0.5), opacity=0.8))\n",
    "\n",
    "    for edge in G.edges():\n",
    "        x0, y0 = list(pos[edge[0]])\n",
    "        x1, y1 = list(pos[edge[1]])\n",
    "        z0, z1 = 0, 0  # Set z positions to 0\n",
    "        edge_trace['x'] = list(edge_trace['x']) + [x0, x1, None]\n",
    "        edge_trace['y'] = list(edge_trace['y']) + [y0, y1, None]\n",
    "        edge_trace['z'] = list(edge_trace['z']) + [z0, z1, None]\n",
    "\n",
    "    for node in G.nodes():\n",
    "        x, y = pos[node]\n",
    "        z = 0  # Set z position to 0\n",
    "        node_trace['x'] = list(node_trace['x']) + [x]\n",
    "        node_trace['y'] = list(node_trace['y']) + [y]\n",
    "        node_trace['z'] = list(node_trace['z']) + [z]\n",
    "        node_trace['marker']['color'] = list(node_trace['marker']['color']) + [G.nodes[node]['size']]\n",
    "\n",
    "    fig = go.Figure(data=[edge_trace, node_trace],\n",
    "                    layout=go.Layout(\n",
    "                        title=username+' - network graphs topics',\n",
    "                        titlefont=dict(size=16),\n",
    "                        showlegend=False,\n",
    "                        hovermode='closest',\n",
    "                        margin=dict(b=20,l=5,r=5,t=40),\n",
    "                        scene=dict(\n",
    "                            xaxis=dict(title='', showticklabels=False, showgrid=False, zeroline=False),\n",
    "                            yaxis=dict(title='', showticklabels=False, showgrid=False, zeroline=False),\n",
    "                            zaxis=dict(title='', showticklabels=False, showgrid=False, zeroline=False),\n",
    "                            camera=dict(up=dict(x=0, y=0, z=1), eye=dict(x=-1.5, y=1.5, z=1.5))\n",
    "                        )\n",
    "                    ))\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def topic_modeling2(username):\n",
    "    df = pd.read_csv(username+\"_tweets.csv\")\n",
    "\n",
    "    # Preprocess the tweet text\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    def preprocess_text(text):\n",
    "        # Convert to lowercase\n",
    "        text = text.lower()\n",
    "        # Tokenize the text\n",
    "        tokens = word_tokenize(text)\n",
    "        # Remove stop words\n",
    "        tokens = [token for token in tokens if token not in stop_words]\n",
    "        # Remove punctuation\n",
    "        tokens = [token for token in tokens if token.isalpha()]\n",
    "        return tokens\n",
    "\n",
    "    # Tokenize and preprocess the tweet text\n",
    "    df['tokens'] = df['tweet_text'].apply(preprocess_text)\n",
    "\n",
    "    # Create a dictionary of all the tokens\n",
    "    dictionary = corpora.Dictionary(df['tokens'])\n",
    "\n",
    "    # Filter out low-frequency and high-frequency words\n",
    "    dictionary.filter_extremes(no_below=5, no_above=0.5)\n",
    "\n",
    "    # Create a corpus of bag-of-words for each tweet\n",
    "    corpus = [dictionary.doc2bow(doc) for doc in df['tokens']]\n",
    "\n",
    "\n",
    "\n",
    "    # Build a Latent Dirichlet Allocation (LDA) model with 10 topics\n",
    "    lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=dictionary, num_topics=10, passes=10)\n",
    "\n",
    "\n",
    "    for i, topic in lda_model.show_topics(num_topics=-1, num_words=10, formatted=False):\n",
    "        topic_label = \"S\" + str(i)\n",
    "        print  (\"Topic:\" +topic_label)\n",
    "        for word, weight in topic:\n",
    "            print(\"Word: \"+word + \", wheight:  \"+ str(weight))\n",
    "\n",
    "    # Create a network graph of topics and their top words\n",
    "    G = nx.Graph()\n",
    "    for i, topic in lda_model.show_topics(num_topics=-1, num_words=10, formatted=False):\n",
    "        topic_label = \"S\" + str(i)\n",
    "        G.add_node(topic_label, size=500, color='red')\n",
    "        for word, weight in topic:\n",
    "            word_label = word\n",
    "            G.add_node(word_label, size=weight*500, color='blue')\n",
    "            G.add_edge(topic_label, word_label, weight=weight)\n",
    "\n",
    "    # Define the node positions using a Fruchterman-Reingold layout\n",
    "    pos = nx.layout.fruchterman_reingold_layout(G)\n",
    "\n",
    "    # Convert the network graph to a pandas dataframe\n",
    "    df = pd.DataFrame(G.edges(data=True), columns=['source', 'target', 'weight'])\n",
    "    df['weight'] = df['weight'].map(lambda x: x['weight'])\n",
    "\n",
    "     # Create an empty graph\n",
    "    G = nx.Graph()\n",
    "\n",
    "    # Add edges to the graph from the dataframe\n",
    "    for index, row in df.iterrows():\n",
    "        G.add_edge(row['source'], row['target'], weight=row['weight'])\n",
    "\n",
    "    color_map = ['#1f77b4' if n.startswith('S') else '#ff7f0e' for n in G.nodes()]\n",
    "    size_map = [300 if n.startswith('S') else 200 for n in G.nodes()]\n",
    "\n",
    "    width_map = [d['weight']*10 for (u,v,d) in G.edges(data=True)]\n",
    "\n",
    "    pos = nx.spring_layout(G, iterations=50)\n",
    "\n",
    "    nx.draw_networkx_nodes(G, pos, node_color=color_map, node_size=size_map )\n",
    "    nx.draw_networkx_edges(G, pos, width=width_map, edge_color='black')\n",
    "    nx.draw_networkx_labels(G, pos, font_size=5, font_family='sans-serif')\n",
    "\n",
    "    edge_labels = {(row['source'], row['target']): '{:.2f}'.format(row['weight']) for index, row in df.iterrows()}\n",
    "    nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_size=5)\n",
    "\n",
    "    plt.axis('off')\n",
    "    plt.title('Topic Modeling Network Graph')\n",
    "\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hashtag_evaluation(username):\n",
    "    df = pd.read_csv(username+'_gnrl_tweets.csv')\n",
    "\n",
    "    # Replace NaN values in hashtags column with empty lists\n",
    "    df['hashtags'].fillna('[]', inplace=True)\n",
    "\n",
    "    # Create a new column for the total number of hashtags in each tweet\n",
    "    df['hashtags_count'] = df['hashtags'].apply(lambda x: len(eval(x)) if isinstance(x, str) else 0)\n",
    "\n",
    "\n",
    "    # Extract all unique hashtags from the dataset and store them in a list\n",
    "    hashtags_list = []\n",
    "    for hashtags in df['hashtags']:\n",
    "        hashtags_list.extend(eval(hashtags))\n",
    "    unique_hashtags = list(set(hashtags_list))\n",
    "    if 'ecb' in unique_hashtags:\n",
    "        unique_hashtags.remove('ecb')\n",
    "    if 'europeancentralbank' in unique_hashtags:\n",
    "        unique_hashtags.remove('europeancentralbank')\n",
    "    if 'ECB' in unique_hashtags:\n",
    "        unique_hashtags.remove('ECB')\n",
    "    if 'fed' in unique_hashtags:\n",
    "        unique_hashtags.remove('fed')\n",
    "    if 'federalreserve' in unique_hashtags:\n",
    "        unique_hashtags.remove('federalreserve')\n",
    "    if 'FederalReserve' in unique_hashtags:\n",
    "        unique_hashtags.remove('FederalReserve')\n",
    "    if 'BankofEngland' in unique_hashtags:\n",
    "        unique_hashtags.remove('BankofEngland')\n",
    "    if 'FED' in unique_hashtags:\n",
    "        unique_hashtags.remove('FED')\n",
    "    if 'boe' in unique_hashtags:\n",
    "        unique_hashtags.remove('boe')\n",
    "    if 'BoE' in unique_hashtags:\n",
    "        unique_hashtags.remove('BoE')\n",
    "    if 'bankofengland' in unique_hashtags:\n",
    "        unique_hashtags.remove('bankofengland')\n",
    "    if 'Fed' in unique_hashtags:\n",
    "        unique_hashtags.remove('Fed')\n",
    "    if 'BOE' in unique_hashtags:\n",
    "        unique_hashtags.remove('BOE')\n",
    "    if 'BankOfEngland' in unique_hashtags:\n",
    "        unique_hashtags.remove('BankOfEngland')\n",
    "    if 'Bank0fEngland' in unique_hashtags:\n",
    "        unique_hashtags.remove('BankofEngland')\n",
    "\n",
    "    # Create a new dataframe to store the count of each hashtag in the entire dataset\n",
    "    hashtag_counts = pd.DataFrame(columns=['hashtag', 'count'])\n",
    "    for hashtag in unique_hashtags:\n",
    "        count = hashtags_list.count(hashtag)\n",
    "        hashtag_counts = hashtag_counts.append({'hashtag': hashtag, 'count': count}, ignore_index=True)\n",
    "\n",
    "    # Create a bar chart to visualize the most popular hashtags in the dataset\n",
    "    top_hashtags = hashtag_counts.sort_values('count', ascending=False).head(10)\n",
    "    sns.barplot(x='hashtag', y='count', data=top_hashtags)\n",
    "    plt.title('Top 10 Hashtags')\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.show()\n",
    "\n",
    "    # Calculate the frequency of each hashtag over time\n",
    "    df['date'] = pd.to_datetime(df['tweet_date'])\n",
    "    hashtag_freq = pd.DataFrame(columns=['date'] + unique_hashtags)\n",
    "    for date, group in df.groupby(df['date'].dt.date):\n",
    "        date_hashtags = []\n",
    "        for hashtags in group['hashtags']:\n",
    "            date_hashtags.extend(eval(hashtags))\n",
    "        date_hashtags_count = Counter(date_hashtags)\n",
    "        date_hashtags_count['date'] = date\n",
    "        hashtag_freq = hashtag_freq.append(date_hashtags_count, ignore_index=True)\n",
    "\n",
    "    # Create a line graph to visualize the trend of each hashtag over time\n",
    "    hashtag_freq.set_index('date', inplace=True)\n",
    "    hashtag_freq.plot(figsize=(12, 6))\n",
    "    plt.title('Hashtag Frequency over Time')\n",
    "    plt.legend().set_visible(False)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    # Perform sentiment analysis on the tweets\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    for hashtag in unique_hashtags:\n",
    "        hashtag_df = df[df['hashtags'].str.contains(hashtag)]\n",
    "        hashtag_df['sentiment'] = hashtag_df['tweet_text'].apply(lambda x: analyzer.polarity_scores(x)['compound'])\n",
    "        mean_sentiment = hashtag_df['sentiment'].mean()\n",
    "        print('Hashtag:', hashtag)\n",
    "        print('Mean Sentiment:', mean_sentiment)\n",
    "\n",
    "    # Create a scatter plot to visualize the relationship between hashtag frequency and sentiment score\n",
    "    hashtag_sentiment = pd.DataFrame(columns=['hashtag', 'frequency', 'sentiment'])\n",
    "    for hashtag in unique_hashtags:\n",
    "        hashtag_df = df[df['hashtags'].str.contains(hashtag)]\n",
    "        frequency = hashtag_df.shape[0]\n",
    "        sentiment = hashtag_df['tweet_text'].apply(lambda x: analyzer.polarity_scores(x)['compound']).mean()\n",
    "        hashtag_sentiment = hashtag_sentiment.append({'hashtag': hashtag, 'frequency': frequency, 'sentiment': sentiment}, ignore_index=True)\n",
    "\n",
    "    sns.scatterplot(data=hashtag_sentiment, x='frequency', y='sentiment', hue='hashtag')\n",
    "    plt.title('Hashtag Frequency vs. Sentiment')\n",
    "    plt.legend().set_visible(False)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def correlations1 ():\n",
    "\n",
    "\n",
    "    tweets = pd.concat([ecb_tweets, boe_tweets, fed_tweets])\n",
    "\n",
    "    # Drop the unnecessary columns\n",
    "    tweets = tweets.drop(['conversation_id', 'cash_tags', 'quoted_tweet', 'vibe', 'card'], axis=1)\n",
    "\n",
    "    # Add a column for hashtag count\n",
    "    tweets['hashtag_count'] = tweets['hashtags'].str.count(',')+1\n",
    "\n",
    "    # Load the sentiment scores for each tweet\n",
    "    sentiment_scores = pd.read_csv('sentiment_scores.csv')\n",
    "\n",
    "    # Merge the sentiment scores into the tweets DataFrame\n",
    "    tweets = pd.merge(tweets, sentiment_scores, on='tweet_text')\n",
    "\n",
    "    # Add a column for engagement ratio\n",
    "    tweets['engagement_ratio'] = (tweets['retweet_count'] + tweets['like_count'] + tweets['view_count']) / tweets['like_count']\n",
    "\n",
    "    # Compute the correlation matrix\n",
    "    corr = tweets.corr()\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    sns.heatmap(corr, cmap='Blues', center=0, annot=True, fmt='.2f', ax=ax)\n",
    "    ax.set_title('Correlation Plot of Twitter Data')\n",
    "    plt.show()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from langdetect import detect, LangDetectException\n",
    "def correlations2 ():\n",
    "    # Define the detect_language function\n",
    "    def detect_language(text):\n",
    "        try:\n",
    "            return detect(text)\n",
    "        except LangDetectException:\n",
    "            return 'unknown'\n",
    "\n",
    "\n",
    "    tweets = pd.concat([ecb_tweets, boe_tweets, fed_tweets])\n",
    "\n",
    "    tweets = tweets.drop(['conversation_id', 'cash_tags', 'quoted_tweet', 'vibe', 'card'], axis=1)\n",
    "\n",
    "    tweets['hashtag_count'] = tweets['hashtags'].str.count('#')\n",
    "\n",
    "    sentiment_scores = pd.read_csv('sentiment_scores.csv')\n",
    "\n",
    "    tweets = pd.merge(tweets, sentiment_scores, on='tweet_text')\n",
    "\n",
    "    tweets['engagement_ratio'] = (tweets['retweet_count'] + tweets['like_count'] + tweets['view_count']) / tweets['like_count']\n",
    "\n",
    "    # Add columns for mentioned count, language, and username\n",
    "    tweets['mentioned_count'] = tweets['mentionned_users'].str.count('@')\n",
    "    tweets['language'] = tweets['tweet_text'].apply(lambda x: detect_language(x))\n",
    "    tweets['username'] = tweets['username'].apply(lambda x: x.lower())\n",
    "\n",
    "    # Visualize the distribution of mentioned count by sentiment score\n",
    "    sns.boxplot(x='sentiment_score', y='mentioned_count', data=tweets)\n",
    "    plt.title('Distribution of Mentioned Count by Sentiment Score')\n",
    "    plt.show()\n",
    "\n",
    "    # Create a bar chart of the top 10 most frequent languages\n",
    "    top_languages = tweets['language'].value_counts().head(10)\n",
    "    sns.barplot(x=top_languages.values, y=top_languages.index)\n",
    "    plt.title('Top 10 Most Frequent Languages')\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_sentiment():\n",
    "    ecb_tweets = pd.read_csv('ecb_gnrl_tweets.csv')\n",
    "    boe_tweets = pd.read_csv('bankofengland_gnrl_tweets.csv')\n",
    "    fed_tweets = pd.read_csv('federalreserve_gnrl_tweets.csv')\n",
    "\n",
    "    tweets = pd.concat([ecb_tweets, boe_tweets, fed_tweets])\n",
    "\n",
    "    # Define a function to compute sentiment scores\n",
    "    def get_sentiments(text):\n",
    "        blob = TextBlob(text)\n",
    "        return blob.sentiment.polarity\n",
    "\n",
    "    # Apply the function to each tweet in the DataFrame\n",
    "    tweets['sentiment_score'] = tweets['tweet_text'].apply(get_sentiments)\n",
    "\n",
    "    # Save the sentiment scores to a csv file\n",
    "    tweets[['tweet_text', 'sentiment_score']].to_csv('sentiment_scores.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def data_gephi_followers (username):\n",
    "    df = pd.read_csv(username+\"_followers.csv\")\n",
    "\n",
    "    # Create the nodes DataFrame with the desired attributes\n",
    "    nodes_df = pd.DataFrame({'username': df['Username'], 'language': 0, 'isInfluencer': 0, 'profession': 0, 'isBank': 0, 'hashtags': 0})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Create the edges DataFrame with the desired attributes\n",
    "    edges_df = pd.DataFrame({'source': df['Username'], 'target': username, 'type': 'follow', 'weight': 1.5})\n",
    "\n",
    "    # Save the nodes and edges DataFrames to CSV files\n",
    "    nodes_df.to_csv(username+'_followers_nodes.csv', index=False)\n",
    "    edges_df.to_csv(username+'_followers_edges.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_gephi_tweet(username, general):\n",
    "\n",
    "    import ast\n",
    "    if (general == 0):\n",
    "        df = pd.read_csv(username+\"_tweets.csv\")\n",
    "    else:\n",
    "        df = pd.read_csv(username+\"_gnrl_tweets.csv\")\n",
    "\n",
    "    # Initialize empty lists to store nodes and edges\n",
    "    nodes = []\n",
    "    edges = []\n",
    "\n",
    "    def classify_language(text):\n",
    "        try:\n",
    "            lang = detect(text)\n",
    "        except:\n",
    "            lang = \"unknown\"\n",
    "        return lang\n",
    "\n",
    "    # Define a function to classify user profession based on keywords in tweet text\n",
    "    def classify_profession(text):\n",
    "        keywords = {\n",
    "            \"politics\": [\"politics\", \"election\", \"vote\", \"government\"],\n",
    "            \"analyst\": [\"analysis\", \"research\", \"report\", \"data\"],\n",
    "            \"finance expert\": [\"finance\", \"economy\", \"market\", \"investment\"],\n",
    "            \"supporter\": [\"support\", \"advocate\", \"promote\", \"help\"],\n",
    "            \"critic\": [\"criticize\", \"oppose\", \"doubt\", \"question\"],\n",
    "            \"curious\": [\"curious\", \"learn\", \"explore\", \"discover\"],\n",
    "            \"conspiracy theorist\": [\"conspiracy\", \"hoax\", \"fake\", \"misinformation\"]\n",
    "        }\n",
    "        for prof, words in keywords.items():\n",
    "            for word in words:\n",
    "                if word in text.lower():\n",
    "                    return prof\n",
    "        return \"unknown\"\n",
    "\n",
    "    # Iterate through each row in the dataframe\n",
    "    for i, row in df.iterrows():\n",
    "        # Create a node for the user who made the tweet\n",
    "        user = {\n",
    "            \"username\": row[\"username\"],\n",
    "            \"language\": classify_language(row[\"tweet_text\"]),\n",
    "            \"isInfluencer\": 1 if row[\"like_count\"] > 1000 else 0,\n",
    "            \"profession\": classify_profession(row[\"tweet_text\"]),\n",
    "            \"isBank\": 0,\n",
    "            \"isViral\": 1 if row[\"retweet_count\"] + row[\"like_count\"] + row[\"view_count\"] > 10000 else 0,\n",
    "            \"hashtags\": row[\"hashtags\"]\n",
    "        }\n",
    "        if user not in nodes:\n",
    "            nodes.append(user)\n",
    "\n",
    "        mentionned_users = row[\"mentionned_users\"]\n",
    "        try:\n",
    "            mentioned_users = re.findall(\"username='(.*?)'\", mentionned_users)\n",
    "        except  TypeError:\n",
    "            mentioned_users = []\n",
    "\n",
    "\n",
    "\n",
    "        for mention in mentioned_users:\n",
    "            user = {\"username\": mention.strip(), \"language\": 0, \"isInfluencer\": 0, \"profession\": 0, \"isBank\": 0, \"isViral\": 0, \"hashtags\": 0}\n",
    "            if user not in nodes:\n",
    "                nodes.append(user)\n",
    "\n",
    "        # Create an edge between the user who made the tweet and the Bank related\n",
    "        edge = {\"source\": row[\"username\"], \"target\": username, \"type\": \"tweet\", \"weight\": 1.0}\n",
    "        if edge not in edges:\n",
    "            edges.append(edge)\n",
    "\n",
    "        # Create an edge for each user mentioned in the tweet\n",
    "        for mention in mentioned_users:\n",
    "            edge = {\"source\": row[\"username\"], \"target\": mention.strip(), \"type\": \"mention\", \"weight\": 0.5}\n",
    "            if edge not in edges:\n",
    "                edges.append(edge)\n",
    "\n",
    "    # Write nodes to CSV file\n",
    "    df_nodes = pd.DataFrame(nodes)\n",
    "    if(general == 0):\n",
    "        df_nodes.to_csv(username+\"_tweets_nodes.csv\", index=False)\n",
    "    else:\n",
    "        df_nodes.to_csv(username+\"_gnrl_tweets_nodes.csv\", index=False)\n",
    "\n",
    "\n",
    "    # Write edges to CSV file\n",
    "    df_edges = pd.DataFrame(edges)\n",
    "    if(general == 0):\n",
    "        df_edges.to_csv(username+\"_tweets_edges.csv\", index=False)\n",
    "    else:\n",
    "        df_edges.to_csv(username+\"_gnrl_tweets_edges.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def final_gephi_data() :\n",
    "    df0 = pd.read_csv(\"bankofengland_followers_edges.csv\")\n",
    "    df1 = pd.read_csv(\"bankofengland_gnrl_tweets_edges.csv\")\n",
    "    df2 = pd.read_csv(\"bankofengland_tweets_edges.csv\")\n",
    "\n",
    "    df3 = pd.read_csv(\"ecb_followers_edges.csv\")\n",
    "    df4 = pd.read_csv(\"ecb_gnrl_tweets_edges.csv\")\n",
    "    df5 = pd.read_csv(\"ecb_tweets_edges.csv\")\n",
    "\n",
    "\n",
    "    df6 = pd.read_csv(\"federalreserve_followers_edges.csv\")\n",
    "    df7 = pd.read_csv(\"federalreserve_gnrl_tweets_edges.csv\")\n",
    "    df8 = pd.read_csv(\"federalreserve_tweets_edges.csv\")\n",
    "\n",
    "    final_edges = [df0,df1,df2,df3,df4,df5,df6,df7,df8]\n",
    "\n",
    "    final_edges_df = pd.concat(final_edges)\n",
    "\n",
    "    final_edges_df.to_csv(\"final_edges.csv\", index=False)\n",
    "\n",
    "    df0 = pd.read_csv(\"bankofengland_followers_nodes.csv\")\n",
    "    df1 = pd.read_csv(\"bankofengland_gnrl_tweets_nodes.csv\")\n",
    "    df2 = pd.read_csv(\"bankofengland_tweets_nodes.csv\")\n",
    "\n",
    "    df3 = pd.read_csv(\"ecb_followers_nodes.csv\")\n",
    "    df4 = pd.read_csv(\"ecb_gnrl_tweets_nodes.csv\")\n",
    "    df5 = pd.read_csv(\"ecb_tweets_nodes.csv\")\n",
    "\n",
    "\n",
    "    df6 = pd.read_csv(\"federalreserve_followers_nodes.csv\")\n",
    "    df7 = pd.read_csv(\"federalreserve_gnrl_tweets_nodes.csv\")\n",
    "    df8 = pd.read_csv(\"federalreserve_tweets_nodes.csv\")\n",
    "\n",
    "    final_nodes = [df0,df1,df2,df3,df4,df5,df6,df7,df8]\n",
    "\n",
    "    final_nodes_df = pd.concat(final_nodes)\n",
    "\n",
    "    final_nodes_df = final_nodes_df.drop_duplicates(subset=[\"username\"])\n",
    "\n",
    "    final_nodes_df.to_csv(\"final_nodes.csv\", index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data_gephi_tweet(\"ecb\", 0)\n",
    "data_gephi_tweet(\"bankofengland\", 0)\n",
    "data_gephi_tweet(\"federalreserve\", 0)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data_gephi_tweet(\"ecb\", 1)\n",
    "data_gephi_tweet(\"bankofengland\", 1)\n",
    "data_gephi_tweet(\"federalreserve\", 1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data_gephi_followers(\"ecb\")\n",
    "data_gephi_followers(\"bankofengland\")\n",
    "data_gephi_followers(\"federalreserve\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "final_gephi_data()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Correlation data obtained"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "get_sentiment()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "correlations1()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "correlations2()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentiment Analysis - from official account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_analysis(\"bankofengland\", 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_analysis(\"federalreserve\", 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_analysis(\"ecb\", 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentiment Analysis - from official account and related hashtag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_analysis(\"bankofengland\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_analysis(\"federalreserve\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_analysis(\"ecb\", 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "topic_modeling2(\"ecb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_modeling2(\"bankofengland\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_modeling2(\"federalreserve\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "topic_modeling_all()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hashtag Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtag_evaluation(\"ecb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtag_evaluation(\"bankofengland\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtag_evaluation(\"federalreserve\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**Case Study: Whatever it takes**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "15 Days Pre-Speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "get_tweets_csv(\"\",\"2012-07-20\",9,5,2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_analysis(\"15_days_before\", 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "15 Days After-Speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_tweets_csv(\"\",\"2012-07-30\",4,10,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sentiment_analysis(\"15_days_after\", 0)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the two CSV files into Pandas DataFrames\n",
    "df_before = pd.read_csv('15_days_before_tweets.csv')\n",
    "df_after = pd.read_csv('15_days_after_tweets.csv')\n",
    "\n",
    "# Extract the tweet text and tweet date columns\n",
    "df_before = df_before[['tweet_text', 'tweet_date']]\n",
    "df_after = df_after[['tweet_text', 'tweet_date']]\n",
    "\n",
    "# Perform sentiment analysis on the tweet text using TextBlob\n",
    "df_before['sentiment'] = df_before['tweet_text'].apply(lambda x: TextBlob(x).sentiment.polarity)\n",
    "df_after['sentiment'] = df_after['tweet_text'].apply(lambda x: TextBlob(x).sentiment.polarity)\n",
    "\n",
    "# Merge the two DataFrames and add a column indicating whether the tweet is from before or after the ECB event\n",
    "df_before['period'] = 'before'\n",
    "df_after['period'] = 'after'\n",
    "df_merged = pd.concat([df_before, df_after])\n",
    "\n",
    "# Group the merged DataFrame by date and calculate the mean sentiment score for each date\n",
    "df_grouped = df_merged.groupby('tweet_date').mean().reset_index()\n",
    "\n",
    "# Plot the mean sentiment scores against the date, using different colors for the two periods\n",
    "fig, ax = plt.subplots()\n",
    "before = df_grouped[df_grouped['tweet_date'].between('2012-07-10', '2012-07-23')]\n",
    "after = df_grouped[df_grouped['tweet_date'].between('2012-07-25', '2012-08-07')]\n",
    "ax.plot(before['tweet_date'], before['sentiment'], color='blue', label='Before ECB event')\n",
    "ax.plot(after['tweet_date'], after['sentiment'], color='red', label='After ECB event')\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('Sentiment score')\n",
    "ax.set_title('Sentiment trend during two 15-day periods')\n",
    "ax.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correlation with stock Market"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "df_before = pd.read_csv('15_days_before_tweets.csv')\n",
    "df_after = pd.read_csv('15_days_after_tweets.csv')\n",
    "\n",
    "df_before = df_before[['tweet_text', 'tweet_date']]\n",
    "df_after = df_after[['tweet_text', 'tweet_date']]\n",
    "\n",
    "# Perform sentiment analysis on the tweet text using TextBlob\n",
    "df_before['sentiment'] = df_before['tweet_text'].apply(lambda x: TextBlob(x).sentiment.polarity)\n",
    "df_after['sentiment'] = df_after['tweet_text'].apply(lambda x: TextBlob(x).sentiment.polarity)\n",
    "\n",
    "# Create a figure with two subplots, one for each 15-day period\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 5), sharey=True)\n",
    "\n",
    "# Create a boxplot for the first 15-day period\n",
    "df_before_box = df_before[df_before['tweet_date'].between('2012-07-10', '2012-07-23')]\n",
    "axs[0].boxplot(df_before_box['sentiment'], labels=['Before ECB event'], showmeans=True, meanline=True)\n",
    "axs[0].set_title('Sentiment during the first 15-day period')\n",
    "axs[0].set_ylabel('Sentiment score')\n",
    "\n",
    "# Create a boxplot for the second 15-day period\n",
    "df_after_box = df_after[df_after['tweet_date'].between('2012-07-25', '2012-08-07')]\n",
    "axs[1].boxplot(df_after_box['sentiment'], labels=['After ECB event'], showmeans=True, meanline=True)\n",
    "axs[1].set_title('Sentiment during the second 15-day period')\n",
    "\n",
    "# Set the colors of the boxplots\n",
    "colors = ['blue', 'red']\n",
    "for i in range(len(axs)):\n",
    "    axs[i].spines['top'].set_visible(False)\n",
    "    axs[i].spines['right'].set_visible(False)\n",
    "    axs[i].spines['bottom'].set_color(colors[i])\n",
    "    axs[i].spines['left'].set_color(colors[i])\n",
    "    axs[i].tick_params(axis='x', colors=colors[i])\n",
    "    axs[i].tick_params(axis='y', colors=colors[i])\n",
    "    axs[i].grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    for j in range(len(axs[i].lines)):\n",
    "        axs[i].lines[j].set_color(colors[i])\n",
    "\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "df_before = pd.read_csv('15_days_before_tweets.csv')\n",
    "df_after = pd.read_csv('15_days_after_tweets.csv')\n",
    "\n",
    "df_before = df_before[['tweet_text', 'tweet_date']]\n",
    "df_after = df_after[['tweet_text', 'tweet_date']]\n",
    "\n",
    "# Perform sentiment analysis on the tweet text using TextBlob\n",
    "df_before['sentiment'] = df_before['tweet_text'].apply(lambda x: TextBlob(x).sentiment.polarity)\n",
    "df_after['sentiment'] = df_after['tweet_text'].apply(lambda x: TextBlob(x).sentiment.polarity)\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(12, 6), sharey=True)\n",
    "\n",
    "df_before_box = df_before[df_before['tweet_date'].between('2012-07-10', '2012-07-23')]\n",
    "axs[0].boxplot(df_before_box['sentiment'], labels=['Before ECB event'], showmeans=True, meanline=True, sym='.')\n",
    "axs[0].set_title('Sentiment during the first 15-day period')\n",
    "axs[0].set_ylabel('Sentiment score')\n",
    "\n",
    "df_after_box = df_after[df_after['tweet_date'].between('2012-07-25', '2012-08-07')]\n",
    "axs[1].boxplot(df_after_box['sentiment'], labels=['After ECB event'], showmeans=True, meanline=True, sym='.')\n",
    "axs[1].set_title('Sentiment during the second 15-day period')\n",
    "\n",
    "# Set the colors of the boxplots\n",
    "colors = ['blue', 'red']\n",
    "for i in range(len(axs)):\n",
    "    axs[i].spines['top'].set_visible(False)\n",
    "    axs[i].spines['right'].set_visible(False)\n",
    "    axs[i].spines['bottom'].set_color(colors[i])\n",
    "    axs[i].spines['left'].set_color(colors[i])\n",
    "    axs[i].tick_params(axis='x', colors=colors[i])\n",
    "    axs[i].tick_params(axis='y', colors=colors[i])\n",
    "    axs[i].grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    for j in range(len(axs[i].lines)):\n",
    "        axs[i].lines[j].set_color(colors[i])\n",
    "\n",
    "plt.subplots_adjust(wspace=0.2)\n",
    "\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
